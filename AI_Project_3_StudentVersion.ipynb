{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9195d27b",
      "metadata": {
        "id": "9195d27b"
      },
      "source": [
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl align=center>\n",
        "<br>\n",
        "<img src=\"https://aut.ac.ir/templates/tmpl_modern01/images/logo_fa.png\" alt=\"Amirkabir University Logo\" width=\"100\">\n",
        "<br>\n",
        "<font size=6>\n",
        "<b>Ÿæÿ±Ÿà⁄òŸá ÿ≥ŸàŸÖ ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å</b>\n",
        "<br>\n",
        "<font size=5> ÿßÿ≥ÿ™ÿßÿØ ÿØÿ±ÿ≥: ÿØ⁄©ÿ™ÿ± ŸÖŸáÿØ€å ŸÇÿ∑ÿπ€å Ÿà ÿ¢ŸÇÿß€å ÿ®ŸáŸÜÿßŸÖ €åŸàÿ≥ŸÅ€å‚ÄåŸÖŸáÿ±\n",
        "<br>\n",
        "<font size=5> ÿ∑ÿ±ÿßÿ≠ÿßŸÜ Ÿæÿ±Ÿà⁄òŸá: ÿß€åŸÑ€åÿß ÿßÿ≥ÿØ€å ÿå ŸÖÿ≠ŸÖÿØÿ±ÿ∂ÿß ÿ¥€åÿÆ ÿßŸÑÿßÿ≥ŸÑÿßŸÖ€å ÿå ÿ≥€åÿØ ÿ≥€åŸÜÿß ŸÜ⁄ØŸáÿ®ÿßŸÜ ÿå ÿ¢ÿ±€åŸÜ ÿ¨ÿπŸÅÿ±€å\n",
        "<br>\n",
        "<font size=4> Ÿæÿß€å€åÿ≤ €±€¥€∞€¥\n",
        "<hr>\n",
        "</div>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d43ff282",
      "metadata": {
        "id": "d43ff282"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zdUgrlgGkNQZ",
      "metadata": {
        "id": "zdUgrlgGkNQZ"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b7a8f59",
      "metadata": {
        "id": "3b7a8f59"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import opendatasets as od\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfa49d0f",
      "metadata": {
        "id": "bfa49d0f"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "You have to use the `opendatasets` library to download and then use it in either Google Colab or your local Jupyter Notebook\n",
        "\n",
        "You can do the following steps:\n",
        "- Create a Kaggle account\n",
        "- Go to the Setting\n",
        "- Find the API section\n",
        "- Select **Create New Token**\n",
        "- Now you have `Kaggle.json` file, which provides you your \"kaggle username\" and \"kaggle key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e113af",
      "metadata": {
        "id": "20e113af"
      },
      "outputs": [],
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/priyamchoksi/100000-diabetes-clinical-dataset/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5281e17",
      "metadata": {
        "id": "b5281e17"
      },
      "outputs": [],
      "source": [
        "# read the csv file and make a data frame\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b66b0061",
      "metadata": {
        "id": "b66b0061"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- Give a strong reason that why for each sample, only one value amongst African-American, Asian, Caucasian, Hispanic and Other races, is 1? Furthermore, explain are there any other ways to show all these 5 columns together?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62eee9ec",
      "metadata": {
        "id": "62eee9ec"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d24762aa",
      "metadata": {
        "id": "d24762aa"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f52868e",
      "metadata": {
        "id": "2f52868e"
      },
      "source": [
        "### Explore the Dataset\n",
        "\n",
        "Now that we've loaded the dataset, it's time to explore it!\n",
        "<br/>\n",
        "In Data Science, Exploratory Data Analysis (EDA) is the process of analyzing datasets to summarize their main characteristics‚Äîoften using statistical graphics, plots, and other visualization tools.\n",
        "<br/>\n",
        "<br/>\n",
        "EDA typically involves two main parts:\n",
        "- Numerical statistics\n",
        "- Visual analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67525d92",
      "metadata": {
        "id": "67525d92"
      },
      "source": [
        "### Numerical statistics\n",
        "\n",
        "describe the data, examine are there any missing values and find data types for each feature( you should use one cell for each of these tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b25bbbcf",
      "metadata": {
        "id": "b25bbbcf"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "777956cb",
      "metadata": {
        "id": "777956cb"
      },
      "source": [
        "### Data Visualization and Visual Analysis\n",
        "\n",
        "Now, we need to find some relations between features as well as between the target column and features. This helps us understand the shape of the data, detect outliers, and identify potential data quality issues or patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5703a45e",
      "metadata": {
        "id": "5703a45e"
      },
      "source": [
        "#### How to detect whether our data is `Imbalanced` or not?\n",
        "\n",
        "First, you have to count the number of samples in each class, then plot both `pie chart` and `count plot` to achieve a visualization of what you did"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23cac8ac",
      "metadata": {
        "id": "23cac8ac"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f1538f",
      "metadata": {
        "id": "10f1538f"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc38c4eb",
      "metadata": {
        "id": "dc38c4eb"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fd95389",
      "metadata": {
        "id": "0fd95389"
      },
      "source": [
        "#### Exploring the relation between gender and diabetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d536a6",
      "metadata": {
        "id": "03d536a6"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00a38c3f",
      "metadata": {
        "id": "00a38c3f"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- What percentage of men have Diabetes?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f5861bd",
      "metadata": {
        "id": "9f5861bd"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "530a5e8d",
      "metadata": {
        "id": "530a5e8d"
      },
      "source": [
        "#### Exploring the relation between age and diabetes(Age Distribution by Diabetes Status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c605ff35",
      "metadata": {
        "id": "c605ff35"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "874ceff4",
      "metadata": {
        "id": "874ceff4"
      },
      "source": [
        "#### Exploring the relation between BMI and diabetes, hbA1c and diabetes and blood Glucose and diabetes\n",
        "\n",
        "At this point, We need to find outliers and ommit them from the dataset, compare their medians and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3571bc68",
      "metadata": {
        "id": "3571bc68"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- Which plot is appropriate for this purpose? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52015de8",
      "metadata": {
        "id": "52015de8"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3ff087d",
      "metadata": {
        "id": "b3ff087d"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba540516",
      "metadata": {
        "id": "ba540516"
      },
      "source": [
        "Now, remove outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8462f6db",
      "metadata": {
        "id": "8462f6db"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946bb6ce",
      "metadata": {
        "id": "946bb6ce"
      },
      "source": [
        "#### The relation between hypertension, heart disease and smoking history, with diabetes, based on precentage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "127652bc",
      "metadata": {
        "id": "127652bc"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fef77b2",
      "metadata": {
        "id": "1fef77b2"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- What percentage of samples with Heart Disease are Diabetic?\n",
        "- Based on smoking history, which group of samples has the highest rate of Diabetes?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc0e3cd",
      "metadata": {
        "id": "3cc0e3cd"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f39713e",
      "metadata": {
        "id": "8f39713e"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd3e1f3e",
      "metadata": {
        "id": "dd3e1f3e"
      },
      "source": [
        "#### Exploring the trend of diabetes based on year column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013c6ab3",
      "metadata": {
        "id": "013c6ab3"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3380ac24",
      "metadata": {
        "id": "3380ac24"
      },
      "source": [
        "### Feature Selection and Feature Engineering\n",
        "\n",
        "Based on the previous step, you have to decide which features could be helpful? Moreover, what other featuers could potentially/should be added?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "151ea7d1",
      "metadata": {
        "id": "151ea7d1"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb3a936",
      "metadata": {
        "id": "cfb3a936"
      },
      "source": [
        "#### Encode the Categorical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c29362f",
      "metadata": {
        "id": "4c29362f"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a5505d",
      "metadata": {
        "id": "d3a5505d"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- Explain the reason why you have chosen these encoding methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "410c63e3",
      "metadata": {
        "id": "410c63e3"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f1fbf19",
      "metadata": {
        "id": "9f1fbf19"
      },
      "source": [
        "#### Plot the `Correlation Matrix`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5e6e19",
      "metadata": {
        "id": "ab5e6e19"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6b34437",
      "metadata": {
        "id": "e6b34437"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- What can you find out from this correlation matrix?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a479e95",
      "metadata": {
        "id": "6a479e95"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5326022e",
      "metadata": {
        "id": "5326022e"
      },
      "source": [
        "#### Split data into train set and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8384ebb8",
      "metadata": {
        "id": "8384ebb8"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53158047",
      "metadata": {
        "id": "53158047"
      },
      "source": [
        "#### Resampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c340e6",
      "metadata": {
        "id": "c1c340e6"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d52030e",
      "metadata": {
        "id": "7d52030e"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- Why didn't we resample the data before splitting it into train and test set?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0991692",
      "metadata": {
        "id": "d0991692"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "495dd0ce",
      "metadata": {
        "id": "495dd0ce"
      },
      "source": [
        "#### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "049f27b8",
      "metadata": {
        "id": "049f27b8"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64bd464c",
      "metadata": {
        "id": "64bd464c"
      },
      "source": [
        "\n",
        "\n",
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca80c57",
      "metadata": {
        "id": "cca80c57"
      },
      "source": [
        "\n",
        "Clustering is an **unsupervised learning** technique that aims to **group similar data points** into clusters such that:\n",
        "- Points in the same cluster are similar to each other.\n",
        "- Points in different clusters are as different as possible.\n",
        "\n",
        "Unlike supervised learning, clustering does not require labeled data. It helps uncover hidden patterns, structures, or natural groupings in data.\n",
        "\n",
        "In this notebook, you will perform clustering using:\n",
        "- **K-Means**\n",
        "- **Agglomerative (Hierarchical) Clustering**\n",
        "- **DBSCAN**\n",
        "\n",
        "and evaluate them using various **clustering metrics**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f68cb57c",
      "metadata": {
        "id": "f68cb57c"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- Why might clustering be useful for a medical dataset like the one in this notebook?  \n",
        "- Can you think of a situation where clustering could fail to find meaningful patterns?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b27e70f",
      "metadata": {
        "id": "2b27e70f"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c123f48",
      "metadata": {
        "id": "0c123f48"
      },
      "source": [
        "Before applying clustering algorithms, we often perform dimensionality reduction, and one of the most common techniques is **Principal Component Analysis (PCA)**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c11967e3",
      "metadata": {
        "id": "c11967e3"
      },
      "source": [
        "### PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d394ea27",
      "metadata": {
        "id": "d394ea27"
      },
      "source": [
        "PCA (Principal Component Analysis) is a linear transformation method that reduces the number of features in a dataset while preserving as much variance as possible.\n",
        "It works by identifying the directions (called principal components) along which the data varies the most."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa77058",
      "metadata": {
        "id": "ffa77058"
      },
      "source": [
        "####  Why We Use PCA Here\n",
        "- **Noise reduction:** PCA filters out small variations that might come from noise, helping clustering algorithms to focus on the true structure of the data.\n",
        "\n",
        "- **Improved visualization:** When we reduce dimensions to 2 or 3 components, we can visualize clusters in a 2D/3D plot.\n",
        "\n",
        "- **Efficiency:** Many clustering algorithms (like K-Means) work faster and more accurately when data has fewer, informative dimensions.  \n",
        "- **Better cluster structure:** Helps algorithms detect patterns more clearly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6979c4ec",
      "metadata": {
        "id": "6979c4ec"
      },
      "source": [
        "üëâ **Hint:**  \n",
        "In your code, PCA is applied before clustering, which means you are working with transformed, lower-dimensional data that helps clustering algorithms perform better and visualize results clearly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce the dataset to 2 components using PCA and Visualize the data in a 2D scatter plot, coloring points by Diabetes labels."
      ],
      "metadata": {
        "id": "vxKCiXWv6YYl"
      },
      "id": "vxKCiXWv6YYl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b10c6a",
      "metadata": {
        "id": "53b10c6a"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97889c64",
      "metadata": {
        "id": "97889c64"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- How does reducing the number of dimensions help clustering algorithms?  \n",
        "- What might happen if you skip PCA and apply clustering directly to high-dimensional data?\n",
        "- How much of the total variance is explained by the two principal components?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9da30e69",
      "metadata": {
        "id": "9da30e69"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8622eee",
      "metadata": {
        "id": "c8622eee"
      },
      "source": [
        "After dimensionality reduction, we can group similar data points into clusters.\\\n",
        "Let‚Äôs review the most common clustering algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d2d0e40",
      "metadata": {
        "id": "4d2d0e40"
      },
      "source": [
        "### Clustering Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49a78d6",
      "metadata": {
        "id": "d49a78d6"
      },
      "source": [
        "#### 1. K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33ee7d74",
      "metadata": {
        "id": "33ee7d74"
      },
      "source": [
        "K-Means is one of the simplest and most popular clustering algorithms. It tries to partition the data into `k` clusters by minimizing the sum of squared distances between each point and its cluster center (centroid).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee1631cd",
      "metadata": {
        "id": "ee1631cd"
      },
      "source": [
        "**Steps:**\n",
        "\n",
        "1. Choose the number of clusters `k`.\n",
        "2. Initialize `k` random centroids.\n",
        "3. Assign each point to the nearest centroid.\n",
        "4. Recalculate centroids as the mean of all points in the cluster.\n",
        "5. Repeat until centroids no longer change significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement K-Means clustering algorithm using the PCA-transformed data on both training and test data and visualize the cluster results in 2D"
      ],
      "metadata": {
        "id": "QabuKP8h6RCv"
      },
      "id": "QabuKP8h6RCv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833cc847",
      "metadata": {
        "id": "833cc847"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6cb48b1",
      "metadata": {
        "id": "d6cb48b1"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">**Answer the following questions:**</font>\n",
        "\n",
        "- How could you determine the optimal value of *k*?  \n",
        "  (After answering this question, go to the [Choosing the Number of Clusters](#choosing-the-number-of-clusters) section and implement the **Elbow Method** to find the best *k* for your dataset.)\n",
        "\n",
        "- Why is feature scaling important before using K-Means?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab51bec1",
      "metadata": {
        "id": "ab51bec1"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f718da",
      "metadata": {
        "id": "76f718da"
      },
      "source": [
        "#### 2. Agglomerative Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b271edb2",
      "metadata": {
        "id": "b271edb2"
      },
      "source": [
        "Agglomerative clustering follows a **bottom-up** approach.  \n",
        "Initially, each point is its own cluster, and the algorithm keeps merging the closest clusters until one big cluster remains.\n",
        "You can visualize this process using a **dendrogram**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e38e08e8",
      "metadata": {
        "id": "e38e08e8"
      },
      "source": [
        "**Two main types:**\n",
        "\n",
        "- **Agglomerative:**  Start with each point as a cluster, then merge them.\n",
        "\n",
        "- **Divisive:**  Start with one cluster, then split it.\n",
        "\n",
        "**Linkage methods:**\n",
        "\n",
        "- **Single linkage:** uses the minimum distance between clusters.\n",
        "\n",
        "- **Complete linkage:** uses the maximum distance.\n",
        "\n",
        "- **Average linkage:** uses the mean distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70bb256e",
      "metadata": {
        "id": "70bb256e"
      },
      "source": [
        "üí° **Note:**  \n",
        "Hierarchical clustering can be computationally expensive because it calculates the distance matrix for all samples and repeatedly merges clusters.  \n",
        "If a dataset is large, this process may take a long time or even fail due to memory limits ‚Äî you may try running it on a **smaller subset of the data** to visualize the results faster."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Agglomerative Hierarchical Clustering algorithm using the PCA-transformed data on both training and test data and visualize the cluster results in 2D"
      ],
      "metadata": {
        "id": "w7zdKR5Y6I9j"
      },
      "id": "w7zdKR5Y6I9j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "239d2322",
      "metadata": {
        "id": "239d2322"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc55415",
      "metadata": {
        "id": "dbc55415"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- How does hierarchical clustering differ from K-Means conceptually?  \n",
        "- What can you interpret from the dendrogram structure?\n",
        "- Why might hierarchical clustering take a long time to run on larger datasets?  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f527bb",
      "metadata": {
        "id": "10f527bb"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d75965",
      "metadata": {
        "id": "e3d75965"
      },
      "source": [
        "#### 3. DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51a24088",
      "metadata": {
        "id": "51a24088"
      },
      "source": [
        "**DBSCAN** is a **density-based algorithm** that groups together points that are closely packed, while marking points that lie alone as outliers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08c3600b",
      "metadata": {
        "id": "08c3600b"
      },
      "source": [
        "**Key Parameters:**\n",
        "- `eps`: Maximum distance between neighboring points.  \n",
        "- `min_samples`: Minimum number of neighbors to form a dense region.\n",
        "\n",
        "**How it works:**\n",
        "1. For each point, check how many points are within `eps` distance.\n",
        "2. If there are at least `min_samples` points, mark it as a **core point** and expand the cluster.\n",
        "3. Points reachable from a core point belong to the same cluster.\n",
        "4. Points that don‚Äôt belong to any cluster are considered **noise**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Implement DBSCAN Clustering algorithm using the PCA-transformed data on both training and test data and visualize the cluster results in 2D"
      ],
      "metadata": {
        "id": "-sOlGHix52AB"
      },
      "id": "-sOlGHix52AB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2804d7",
      "metadata": {
        "id": "2f2804d7"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a69983",
      "metadata": {
        "id": "b8a69983"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- Why doesn‚Äôt DBSCAN require specifying the number of clusters (*k*)?  \n",
        "- What happens when `eps` is too small or too large?  \n",
        "- Why might DBSCAN be better than K-Means for irregular data shapes?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc11123e",
      "metadata": {
        "id": "cc11123e"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e74522",
      "metadata": {
        "id": "24e74522"
      },
      "source": [
        "### Clustering Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5044305",
      "metadata": {
        "id": "a5044305"
      },
      "source": [
        "Evaluating clustering results is challenging because we typically don‚Äôt have labels.  \n",
        "Metrics fall into two categories: **Internal Metrics** (no true labels needed) and **External Metrics** (require ground truth labels)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62b2c26",
      "metadata": {
        "id": "b62b2c26"
      },
      "source": [
        "#### 1. Internal Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add9f7c4",
      "metadata": {
        "id": "add9f7c4"
      },
      "source": [
        "Internal metrics evaluate the **structure of the clusters** using only the data itself ‚Äî no true labels are needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6421710c",
      "metadata": {
        "id": "6421710c"
      },
      "source": [
        "##### üßÆ 1. Silhouette Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f585b60",
      "metadata": {
        "id": "9f585b60"
      },
      "source": [
        "Measures how well a point fits within its own cluster compared to others.\n",
        "$$\n",
        "Silhouette = \\frac{b - a}{\\max(a, b)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27b84759",
      "metadata": {
        "id": "27b84759"
      },
      "source": [
        "where:  \n",
        "- `a` = average distance to points in the same cluster  \n",
        "- `b` = average distance to points in the nearest cluster  \n",
        "\n",
        "**Range:** -1 to +1 :  \n",
        "\n",
        "- Values close to **+1** mean well-separated clusters.  \n",
        "- Values near **0** mean overlapping clusters.  \n",
        "-  Negative values mean wrong cluster assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95231c8",
      "metadata": {
        "id": "a95231c8"
      },
      "source": [
        "##### üßÆ 2. Davies‚ÄìBouldin Index (DBI)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12755a85",
      "metadata": {
        "id": "12755a85"
      },
      "source": [
        "Measures the average similarity between each cluster and its most similar one.  \n",
        "A **lower DBI** indicates better clustering ‚Äî it rewards clusters that are compact and far apart.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7754b3f",
      "metadata": {
        "id": "f7754b3f"
      },
      "source": [
        "##### üßÆ 3. Calinski‚ÄìHarabasz Index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b84b6965",
      "metadata": {
        "id": "b84b6965"
      },
      "source": [
        "Also called the **Variance Ratio Criterion**, it compares **between-cluster dispersion** to **within-cluster dispersion**.  \n",
        "Higher values indicate better clustering performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the performance of ALL 3 above clustering algorithms using Silhouette Score, DBI and Calinski‚ÄìHarabasz Index on both train and test dataset"
      ],
      "metadata": {
        "id": "S2vy3gBB5tmF"
      },
      "id": "S2vy3gBB5tmF"
    },
    {
      "cell_type": "markdown",
      "id": "53d37265",
      "metadata": {
        "id": "53d37265"
      },
      "source": [
        "##### Kmeans Internal Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a00460",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "53a00460"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5130ee6a",
      "metadata": {
        "id": "5130ee6a"
      },
      "source": [
        "##### Agglomerative Hierarchical Internal Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2c89ee",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ce2c89ee"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ea97bd",
      "metadata": {
        "id": "26ea97bd"
      },
      "source": [
        "##### DBSCAN Internal Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e5460c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "97e5460c"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7960d4de",
      "metadata": {
        "id": "7960d4de"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- Why is Silhouette Score often preferred for visualization-based clustering evaluation?  \n",
        "- What are the limitations of these internal metrics?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45fbb683",
      "metadata": {
        "id": "45fbb683"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf967c0e",
      "metadata": {
        "id": "bf967c0e"
      },
      "source": [
        "#### 2. External Evaluation Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654f3f6d",
      "metadata": {
        "id": "654f3f6d"
      },
      "source": [
        "When we **have ground-truth labels** (for example, synthetic or labeled datasets), we can evaluate how similar our predicted clusters are to the real labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e391d8",
      "metadata": {
        "id": "92e391d8"
      },
      "source": [
        "##### üßÆ 1. Adjusted Rand Index (ARI)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "becd1330",
      "metadata": {
        "id": "becd1330"
      },
      "source": [
        "Compares the similarity between two clusterings (predicted vs. true labels) by counting how many pairs of points are assigned consistently.\n",
        "\n",
        "- ARI = 1 ‚Üí Perfect agreement.  \n",
        "- ARI = 0 ‚Üí Random labeling.  \n",
        "- ARI < 0 ‚Üí Worse than random.\n",
        "\n",
        "It is ‚Äúadjusted‚Äù for chance, meaning random clustering should give a score near 0."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55d3d59",
      "metadata": {
        "id": "f55d3d59"
      },
      "source": [
        "##### üßÆ 2. Normalized Mutual Information (NMI)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92596053",
      "metadata": {
        "id": "92596053"
      },
      "source": [
        "Measures the amount of **shared information** between predicted clusters and true labels.  \n",
        "It is based on the concept of *mutual information* from information theory.\n",
        "\n",
        "Values range from 0 (no agreement) to 1 (perfect match).\n",
        "\n",
        "Evaluate the performance of ALL 3 above clustering algorithms using ARI and NMI on both train and test dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c0604d",
      "metadata": {
        "id": "59c0604d"
      },
      "source": [
        "##### Kmeans External Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ac36fb",
      "metadata": {
        "id": "55ac36fb"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1de1da4",
      "metadata": {
        "id": "c1de1da4"
      },
      "source": [
        "##### Agglomerative Hierarchical External Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76536a98",
      "metadata": {
        "id": "76536a98"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c834bc73",
      "metadata": {
        "id": "c834bc73"
      },
      "source": [
        "##### DBSCAN External Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "975d71f5",
      "metadata": {
        "id": "975d71f5"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8867ef41",
      "metadata": {
        "id": "8867ef41"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "- When would you use external metrics instead of internal ones?  \n",
        "- Why is it important that external metrics are *label-invariant*?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8fe86d3",
      "metadata": {
        "id": "e8fe86d3"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee3285a",
      "metadata": {
        "id": "cee3285a"
      },
      "source": [
        "<a name=\"choosing-the-number-of-clusters\"></a>\n",
        "### Choosing the Number of Clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6daf9805",
      "metadata": {
        "id": "6daf9805"
      },
      "source": [
        "Choosing the right number of clusters (`k`) is a crucial part of clustering, especially for **K-Means**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c99fc364",
      "metadata": {
        "id": "c99fc364"
      },
      "source": [
        "#### What is the Elbow Method?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04de685",
      "metadata": {
        "id": "f04de685"
      },
      "source": [
        "The **Elbow Method** is a visual technique used to determine the optimal number of clusters.\n",
        "\n",
        "It is based on analyzing the **Within-Cluster Sum of Squares (WCSS)** ‚Äî the total squared distance between each point and its assigned cluster centroid.\n",
        "\n",
        "As the number of clusters increases:\n",
        "- WCSS **decreases** (clusters get smaller and more compact),\n",
        "- But the **rate of improvement** slows down.\n",
        "\n",
        "When plotted, WCSS typically forms a **curve with a sharp bend**, resembling an ‚Äúelbow.‚Äù  \n",
        "The point where the reduction in WCSS starts to slow down is considered the **optimal number of clusters**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607519a3",
      "metadata": {
        "id": "607519a3"
      },
      "source": [
        "#### How the Elbow Method Works:\n",
        "1. Run K-Means with different `k` values (e.g., 1‚Äì10).\n",
        "2. Compute the **inertia** (sum of squared distances of points to their nearest centroid) for each `k`.\n",
        "3. Plot `k` vs. inertia.\n",
        "4. Look for the ‚Äúelbow point‚Äù ‚Äî where the decrease in inertia slows down sharply\n",
        "\n",
        "That point represents a good balance between having too few and too many clusters.\n",
        "\n",
        "Implement the Elbow Method on chart to determine the best value of `k` for your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7eb3c5e",
      "metadata": {
        "id": "a7eb3c5e"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb5e6f3",
      "metadata": {
        "id": "abb5e6f3"
      },
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following question:\n",
        "\n",
        "-  Why does inertia decrease as we increase the number of clusters?  \n",
        "- How can you visually detect the optimal `k` on the Elbow plot?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4678a92",
      "metadata": {
        "id": "a4678a92"
      },
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification on each cluster"
      ],
      "metadata": {
        "id": "rnTJBKX5FFqE"
      },
      "id": "rnTJBKX5FFqE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After clustering our diabetic patients into three groups based on their medical and lifestyle features, we move to the classification phase.\n",
        "The goal of this part is to train separate machine learning classifiers for each cluster, since each group of patients may show different relationships between the features and the target label (e.g., diabetes outcome)."
      ],
      "metadata": {
        "id": "Jzx2kFbFGCqZ"
      },
      "id": "Jzx2kFbFGCqZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Cluster-Based Data for Classification"
      ],
      "metadata": {
        "id": "9Ul_xt1iO0Mt"
      },
      "id": "9Ul_xt1iO0Mt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Before training the classification models, we must separate the dataset according to the cluster labels obtained from the **K-Means** algorithm.\n",
        "Each cluster represents a distinct subgroup of patients, and we will train a separate classifier for each one. Use the **scaled training data** and **not the data resulted from PCA algorithm**."
      ],
      "metadata": {
        "id": "bGBW-vs-PXra"
      },
      "id": "bGBW-vs-PXra"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "i5-BdP4qPmcs"
      },
      "id": "i5-BdP4qPmcs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Algorithms"
      ],
      "metadata": {
        "id": "BmdG-bzSITtn"
      },
      "id": "BmdG-bzSITtn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Evaluation is a critical step in any machine learning pipeline.\n",
        "For each model, you should measure key performance metrics that capture different aspects of model.  \n",
        "\n",
        "For each model (Random Forest, Logistic Regression, SVM, and XGBoost), perform the following steps:\n",
        "\n",
        "1. Compute the evaluation metrics for each cluster using:\n",
        "\n",
        "    - accuracy_score\n",
        "    - precision_score\n",
        "    - recall_score\n",
        "    - f1_score\n",
        "\n",
        "2. Plot a confusion matrix for each cluster to visualize how predictions are distributed among classes.\n",
        "You can use ConfusionMatrixDisplay.from_predictions() from sklearn.metrics.\n",
        "\n",
        "3. Create a bar plot comparing metric values (Accuracy, Precision, Recall, F1) for the three clusters of the same model.\n",
        "\n",
        "\n",
        "<b> <font color='red'>**Note**: From now, in the following sections, for evaluating models, you must not use `average = 'weighted'` parameter in f1_score, precision_score, recall_score and accuracy functions. In other words, DO NOT use 'average' parameter at all. </font></b><br>"
      ],
      "metadata": {
        "id": "UB90stc7HRsP"
      },
      "id": "UB90stc7HRsP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####   1. Logistic Regression\n",
        "\n",
        "Logistic Regression is a supervised machine learning algorithm used for classification tasks.\n",
        "It models the probability that a given input belongs to a particular class by using the logistic (sigmoid) function.  \n",
        "Despite its name, it is not used for regression but for predicting categorical outcomes (such as 0 or 1).\n",
        "It is most effective when there is a linear relationship between the input features and the target variable.\n",
        "\n",
        "Implement a Logistic Regression classifier for each cluster and evaluate it on test data."
      ],
      "metadata": {
        "id": "-0itN_WgK5-2"
      },
      "id": "-0itN_WgK5-2"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "jKZGriBgP_KY"
      },
      "id": "jKZGriBgP_KY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- What is the main assumption behind Logistic Regression regarding the relationship between features and the target variable?\n",
        "- What are some limitations of Logistic Regression when dealing with non-linear data?\n",
        "- What are the formulas for Accuracy, Precision, Recall, F1-score?"
      ],
      "metadata": {
        "id": "PAXimX6amEI5"
      },
      "id": "PAXimX6amEI5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "F1tOqez_mppf"
      },
      "id": "F1tOqez_mppf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Random Forest"
      ],
      "metadata": {
        "id": "Ij-mqQp-R4KI"
      },
      "id": "Ij-mqQp-R4KI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble learning algorithm that combines the results of many decision trees to make a final prediction.  \n",
        "Each tree in the forest is trained on a random subset of the data and features, which helps the model avoid overfitting and improves generalization.  \n",
        "Random Forest can capture non-linear relationships and performs well on both classification and regression problems.\n",
        "\n",
        "Implement a Random Forest classifier for each cluster and evaluate it on test data."
      ],
      "metadata": {
        "id": "p0Lj8EGDSRAx"
      },
      "id": "p0Lj8EGDSRAx"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "KBL4rYNdQanr"
      },
      "id": "KBL4rYNdQanr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- What is the main advantage of Random Forest compared to a single Decision Tree?\n",
        "- How do the parameters n_estimators and max_depth affect bias and variance in Random Forest?\n",
        "- In which situation is a high F1-score more meaningful than a high Accuracy score?"
      ],
      "metadata": {
        "id": "1QmsDM9Fmzra"
      },
      "id": "1QmsDM9Fmzra"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "OAvEUF1DpvuT"
      },
      "id": "OAvEUF1DpvuT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "5WpFAd-GXcmk"
      },
      "id": "5WpFAd-GXcmk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks.\n",
        "It works by finding the optimal hyperplane that best separates the data points of different classes with the maximum margin.  \n",
        "SVM can also handle non-linear data by using kernel functions such as the radial basis function (RBF), which transform the input features into higher-dimensional spaces.\n",
        "\n",
        "Implement a Support Vector Machine (SVM) classifier using the RBF kernel.\n",
        "Train and test the model for each cluster."
      ],
      "metadata": {
        "id": "J7E1H6I9XqWC"
      },
      "id": "J7E1H6I9XqWC"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "d1nfn9KoXip6"
      },
      "id": "d1nfn9KoXip6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- What is the role of the kernel function in SVM?\n",
        "- Why might SVM perform poorly on large datasets?"
      ],
      "metadata": {
        "id": "JbCri3nanMD5"
      },
      "id": "JbCri3nanMD5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "4cr1acb5pxvK"
      },
      "id": "4cr1acb5pxvK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. XGBoost"
      ],
      "metadata": {
        "id": "w4kx9ruYYd-r"
      },
      "id": "w4kx9ruYYd-r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (Extreme Gradient Boosting) is an advanced ensemble algorithm based on the concept of gradient boosting.\n",
        "It builds decision trees sequentially, where each new tree attempts to correct the errors made by the previous ones.  \n",
        "XGBoost is known for its high performance, speed, and efficiency in handling structured or tabular datasets.\n",
        "\n",
        "Implement an XGBoost classifier. Train and test the model for each cluster."
      ],
      "metadata": {
        "id": "Lhheg-gfYzvf"
      },
      "id": "Lhheg-gfYzvf"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "W9jAeeg4y_0Q"
      },
      "id": "W9jAeeg4y_0Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- What type of learning strategy does XGBoost use to improve its performance iteratively?\n",
        "- How does XGBoost handle overfitting through parameters like max_depth and learning_rate?"
      ],
      "metadata": {
        "id": "sYREuv-tnO-2"
      },
      "id": "sYREuv-tnO-2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "bbZRf8GupzrO"
      },
      "id": "bbZRf8GupzrO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregated Model Performance"
      ],
      "metadata": {
        "id": "VrdVii_eH6RE"
      },
      "id": "VrdVii_eH6RE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, you will analyze the overall (aggregated) performance of each classification model across all clusters.\n",
        "Instead of evaluating each cluster separately, you‚Äôll combine all predictions and compute the total metrics."
      ],
      "metadata": {
        "id": "flwVlQuSH-ON"
      },
      "id": "flwVlQuSH-ON"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregated performance helps determine which model performs the best on average, across different subsets (clusters) of the dataset.\n",
        "This is especially useful when:\n",
        "\n",
        "- You want to assess model robustness and stability.  \n",
        "- You want a single representative metric for each model.  \n",
        "- You need to compare models that may perform well on one cluster but poorly on others."
      ],
      "metadata": {
        "id": "tTHUyrm3ICD5"
      },
      "id": "tTHUyrm3ICD5"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "91kIqevRIGZf"
      },
      "id": "91kIqevRIGZf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- Which model achieved the highest overall accuracy when considering all clusters together?\n",
        "- Which model demonstrates the most balanced performance across all metrics (Accuracy, Precision, Recall, and F1-score)?\n"
      ],
      "metadata": {
        "id": "AL-TRUUEIM93"
      },
      "id": "AL-TRUUEIM93"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "B8YJqCTIIQc0"
      },
      "id": "B8YJqCTIIQc0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Comparison\n",
        "\n"
      ],
      "metadata": {
        "id": "n-AyCI_QzE4P"
      },
      "id": "n-AyCI_QzE4P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have trained, tuned, and evaluated several classification models on different patient clusters, it is time to compare their overall performance.\n",
        "The purpose of this section is not to write additional code, but to analyze and interpret the results obtained from the previous evaluations."
      ],
      "metadata": {
        "id": "Gvt-jVaz1tAf"
      },
      "id": "Gvt-jVaz1tAf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- Are the results consistent with what you expected based on each model‚Äôs nature?\n",
        "- How does model interpretability compare between Logistic Regression and XGBoost?\n",
        "- In a medical decision-making setting, which would you prioritize ‚Äî interpretability or predictive performance?\n",
        "- If you were to deploy one model in a healthcare system to predict diabetes outcomes, which model would you choose and why?"
      ],
      "metadata": {
        "id": "wwRm2yP1nWNS"
      },
      "id": "wwRm2yP1nWNS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "qe7wSwxlp1YV"
      },
      "id": "qe7wSwxlp1YV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "wwlXxsac5BMm"
      },
      "id": "wwlXxsac5BMm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every machine learning model has a set of hyperparameters ‚Äî parameters that are not learned directly from the data, but rather control the learning process itself.\n",
        "Examples include the number of trees in a Random Forest, the learning rate in XGBoost, or the regularization strength in Logistic Regression.\n",
        "\n",
        "Choosing optimal hyperparameter values is crucial because they can significantly affect model performance.\n",
        "The process of finding the best combination of these hyperparameters is called hyperparameter tuning."
      ],
      "metadata": {
        "id": "_ZyLVf2z6Yaz"
      },
      "id": "_ZyLVf2z6Yaz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Grid Search"
      ],
      "metadata": {
        "id": "chF6IMer6luJ"
      },
      "id": "chF6IMer6luJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search is a systematic and exhaustive method that evaluates all possible combinations of specified hyperparameter values.  \n",
        "Although it guarantees testing every option, it can be computationally expensive, especially when there are many parameters or large value ranges.\n",
        "Perform Grid Search on clusters considering the parameters in the next cell.\n",
        "Perform Grid Search on the model with the best aggregated performance.\n",
        "\n",
        "Note: The following Grid Search tests a limited range of hyperparameters to reduce computational cost while still allowing the model to explore different\n",
        "tree depths and splitting strategies.After tuning, compare the best parameters and performance across clusters."
      ],
      "metadata": {
        "id": "78d879Xc6v3K"
      },
      "id": "78d879Xc6v3K"
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}"
      ],
      "metadata": {
        "id": "9vGXqShcuWKT"
      },
      "id": "9vGXqShcuWKT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "yAiSvF4JxAxd"
      },
      "id": "yAiSvF4JxAxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- Explain how Grid Search explores the hyperparameter space.\n",
        "- Why can Grid Search become computationally expensive as the number of parameters increases?\n",
        "- When is Grid Search preferable to Randomized Search?"
      ],
      "metadata": {
        "id": "G_c-8HQ7nYqK"
      },
      "id": "G_c-8HQ7nYqK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "91vuI9tOp3oS"
      },
      "id": "91vuI9tOp3oS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Randomized Search"
      ],
      "metadata": {
        "id": "VTfi-DPt6_a-"
      },
      "id": "VTfi-DPt6_a-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomized Search selects a random subset of combinations from the defined hyperparameter grid.  \n",
        "It is faster than Grid Search and can still yield good results, especially when some parameters have less influence on model performance.\n",
        "\n",
        "Perform Randomized Search on the model with the best aggregated performance."
      ],
      "metadata": {
        "id": "edpHjaFh7lqn"
      },
      "id": "edpHjaFh7lqn"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "hos4r3ZlApSf"
      },
      "id": "hos4r3ZlApSf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- What is the main advantage of Randomized Search over Grid Search?\n",
        "- If Randomized Search samples fewer combinations, how does it still maintain reasonable performance?"
      ],
      "metadata": {
        "id": "iwaMwOKvnbQ7"
      },
      "id": "iwaMwOKvnbQ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "vGXyg6VSp6t1"
      },
      "id": "vGXyg6VSp6t1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Bayesian Optimization"
      ],
      "metadata": {
        "id": "W4kIgNhOwAin"
      },
      "id": "W4kIgNhOwAin"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian Optimization is a smarter way of tuning hyperparameters. It builds a model of the performance (objective function) based on past results and decides which hyperparameters to try next by predicting which ones might work better. Libraries like Optuna or Hyperopt help automate this process.\n",
        "Perform Bayesian Optimization on the model with the best aggregated performance"
      ],
      "metadata": {
        "id": "v3FPUT6awJpe"
      },
      "id": "v3FPUT6awJpe"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "cfm_4b-KWKAe"
      },
      "id": "cfm_4b-KWKAe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- In simple terms, how does Bayesian Optimization decide which hyperparameters to try next?\n",
        "- Why can Bayesian Optimization be more efficient than both Grid and Randomized Search?"
      ],
      "metadata": {
        "id": "GukXLvXAwXsv"
      },
      "id": "GukXLvXAwXsv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "59hfTm0-w5ma"
      },
      "id": "59hfTm0-w5ma"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification on the Whole Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "IiP1YI71uuvh"
      },
      "id": "IiP1YI71uuvh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous sections, we applied clustering to identify natural groupings in our diabetes dataset and then trained separate classifiers for each cluster. This approach assumes that different patient subgroups may have distinct risk patterns for diabetes.\n",
        "\n",
        "However, it's also important to evaluate how well our classification models perform when trained on the entire dataset without any clustering preprocessing. This approach treats all patients as a single homogeneous group and can help us understand:\n",
        "\n",
        "- **Baseline Performance**: How well can we predict diabetes using all available features without subgroup analysis?\n",
        "- **Model Comparison**: Which algorithms perform best when given access to the complete feature space?\n",
        "- **Feature Importance**: What are the most predictive features across the entire population?\n",
        "- **Clustering Impact**: How does the cluster-based approach compare to the traditional whole-dataset approach?\n",
        "\n",
        "This section will implement the same classification algorithms (Logistic Regression, Random Forest, SVM, and XGBoost) but train them on the complete preprocessed dataset rather than on individual clusters.\n",
        "\n",
        "We will use the scaled data from the preprocessing section (X_train_resampled_scaled and X_test_scaled) rather than the PCA-transformed data. This allows the models to have access to all original features and their interactions."
      ],
      "metadata": {
        "id": "ofLxjO0kuvFN"
      },
      "id": "ofLxjO0kuvFN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization with Optuna"
      ],
      "metadata": {
        "id": "HercIuMWu1hN"
      },
      "id": "HercIuMWu1hN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous clustering section, we used Grid Search and Randomized Search for hyperparameter tuning. However, these methods have limitations.\n",
        "\n",
        "As mentioned previously, Bayesian Optimization is a smarter way of tuning hyperparameters. **Optuna** is a modern hyperparameter optimization framework that uses **Bayesian Optimization** and **Tree-structured Parzen Estimator (TPE)** algorithms to efficiently find optimal hyperparameters."
      ],
      "metadata": {
        "id": "0lt9cMY_u3Sc"
      },
      "id": "0lt9cMY_u3Sc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Answer the following questions:\n",
        "\n",
        "- What is the main difference between Optuna's TPE algorithm and random search?\n",
        "- Why might Optuna be more efficient than Grid Search for hyperparameter optimization?\n",
        "- In what scenarios would you prefer Optuna over traditional hyperparameter tuning methods?\n"
      ],
      "metadata": {
        "id": "aHhxk_fxu7QT"
      },
      "id": "aHhxk_fxu7QT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "V7kTOuK_u9cs"
      },
      "id": "V7kTOuK_u9cs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Logistic Regression with Optuna\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FcTte_htu_v5"
      },
      "id": "FcTte_htu_v5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Logistic Regression, we need to optimize several key hyperparameters:\n",
        "\n",
        "**Core Parameters:**\n",
        "- **C**: Regularization strength (inverse of regularization parameter)\n",
        "- **solver**: Algorithm to use for optimization\n",
        "- **penalty**: Type of regularization (l1, l2, elasticnet)\n",
        "- **max_iter**: Maximum number of iterations for convergence\n",
        "\n",
        "**Advanced Parameters:**\n",
        "- **l1_ratio**: Mixing parameter for elasticnet penalty (only used with saga solver)\n",
        "\n",
        "**Parameter Compatibility:**\n",
        "- **liblinear**: Supports l1, l2 penalties\n",
        "- **lbfgs**: Supports only l2 penalty  \n",
        "- **saga**: Supports l1, l2, elasticnet penalties"
      ],
      "metadata": {
        "id": "pU3K-gfCvBie"
      },
      "id": "pU3K-gfCvBie"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement Optuna Optimization for Logistic Regression\n"
      ],
      "metadata": {
        "id": "9R0f4J81vDn1"
      },
      "id": "9R0f4J81vDn1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to implement Optuna optimization for Logistic Regression with the following requirements:\n",
        "\n",
        "**Parameter Grid to Optimize:**\n",
        "- **C**: uniform(0.001, 10.0)\n",
        "- **solver**: categorical(['liblinear', 'lbfgs', 'saga'])\n",
        "- **max_iter**: randint(100, 1000)\n",
        "- **penalty**: categorical(['l1', 'l2', 'elasticnet'])\n",
        "- **l1_ratio**: uniform(0.0, 1.0)\n",
        "\n",
        "**Objective Function Requirements:**\n",
        "1. **Parameter Definition**: Use the fixed parameter grid with compatible solver-penalty combinations\n",
        "2. **Solver-Penalty Compatibility**: Ensure only valid combinations are suggested:\n",
        "   - liblinear ‚Üí l1, l2 penalties\n",
        "   - lbfgs ‚Üí l2 penalty only\n",
        "   - saga ‚Üí l1, l2, elasticnet penalties\n",
        "3. **Cross-Validation**: Use 3-fold cross-validation with F1-score as the optimization metric\n",
        "4. **Study Creation**: Create an Optuna study with TPE sampler for 15 trials"
      ],
      "metadata": {
        "id": "scjvaukFvFIu"
      },
      "id": "scjvaukFvFIu"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "WXcY24G1vF9H"
      },
      "id": "WXcY24G1vF9H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and Evaluate the Best Logistic Regression Model"
      ],
      "metadata": {
        "id": "DKaxgbL2vH-e"
      },
      "id": "DKaxgbL2vH-e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to train and evaluate the Optuna-optimized Logistic Regression model with best parameters from the Optuna study and then calculate metrics and display the results. Also plot the confusion matrix."
      ],
      "metadata": {
        "id": "_F6pxv3ZvKFp"
      },
      "id": "_F6pxv3ZvKFp"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "o8AeSiRtvMDK"
      },
      "id": "o8AeSiRtvMDK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">**Logistic Regression with Optuna vs Cluster-Based Classification:**\n",
        "\n",
        "Based on the Logistic Regression results shown above, answer the following questions by comparing with the cluster-based Logistic Regression results from the previous sections:\n",
        "\n",
        "- **Hyperparameter Optimization Impact**: How does using Optuna for hyperparameter optimization compare to using default parameters in the cluster-based Logistic Regression models? What improvements do you observe?\n",
        "\n",
        "- **Performance Comparison**: Compare the accuracy, precision, recall, and F1-scores between:\n",
        "  - The Optuna-optimized Logistic Regression on the whole dataset (results shown above)\n",
        "  - The aggregated performance of cluster-based Logistic Regression models\n",
        "  - Which approach performs better overall and why?\n",
        "\n",
        "- **Optimization Strategy**:\n",
        "  - How might the optimal hyperparameters differ between the whole dataset approach vs. individual clusters?\n",
        "\n",
        "- **Methodology Evaluation**: Given these results, would you recommend using Optuna for hyperparameter optimization in future medical classification tasks? Justify your answer based on the performance improvements observed."
      ],
      "metadata": {
        "id": "M0pgC5XBvNAF"
      },
      "id": "M0pgC5XBvNAF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "iT18jhS3vQF1"
      },
      "id": "iT18jhS3vQF1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Random Forest with Optuna"
      ],
      "metadata": {
        "id": "x-RTgfsevTay"
      },
      "id": "x-RTgfsevTay"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Random Forest, we need to optimize several key hyperparameters:\n",
        "\n",
        "**Core Parameters:**\n",
        "- **n_estimators**: Number of trees in the forest\n",
        "- **max_depth**: Maximum depth of individual trees\n",
        "- **min_samples_split**: Minimum samples required to split an internal node\n",
        "- **min_samples_leaf**: Minimum samples required to be at a leaf node\n",
        "\n",
        "**Advanced Parameters:**\n",
        "- **max_features**: Number of features to consider when looking for the best split\n",
        "- **bootstrap**: Whether bootstrap samples are used when building trees"
      ],
      "metadata": {
        "id": "3ZzTu9e5vWXT"
      },
      "id": "3ZzTu9e5vWXT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement Optuna Optimization for Random Forest"
      ],
      "metadata": {
        "id": "G_SwmrBRvYP4"
      },
      "id": "G_SwmrBRvYP4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to implement Optuna optimization for Random Forest with the following requirements:\n",
        "\n",
        "**Parameter Grid to Optimize:**\n",
        "- **n_estimators**: rcategorical([50, 100, 150, 200])\n",
        "- **max_depth**: categorical([None, 10, 20])  \n",
        "- **min_samples_split**: categorical([2, 5, 10])\n",
        "- **min_samples_leaf**: categorical([1, 2, 4])\n",
        "- **max_features**: categorical(['sqrt', 'log2', None])\n",
        "- **bootstrap**: categorical([True, False])\n",
        "\n",
        "**Implementation Setup:**\n",
        "1. Define the objective function with proper parameter handling\n",
        "3. Cross-Validation: Use 3-fold cross-validation with F1-score as the optimization metric\n",
        "4. Study Creation: Create an Optuna study with TPE sampler for 15 trials"
      ],
      "metadata": {
        "id": "F9tQRCOHvaHb"
      },
      "id": "F9tQRCOHvaHb"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "bl-RfJPqvb2X"
      },
      "id": "bl-RfJPqvb2X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and Evaluate the Best Random Forest Model"
      ],
      "metadata": {
        "id": "2AW0FZksvcds"
      },
      "id": "2AW0FZksvcds"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to train and evaluate the Optuna-optimized Random Forest model with best parameters from the Optuna study and then calculate metrics and display the results. Also plot the confusion matrix."
      ],
      "metadata": {
        "id": "BZt6lYJIveZa"
      },
      "id": "BZt6lYJIveZa"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "oh_2atrgvhrI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oh_2atrgvhrI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">Random Forest with Optuna vs Cluster-Based Classification:\n",
        "\n",
        "Based on the Random Forest results, answer the following questions by comparing with the cluster-based Random Forest results from the previous sections:\n",
        "\n",
        "- **Hyperparameter Optimization Impact**: How does using Optuna for hyperparameter optimization compare to using default parameters in the cluster-based Random Forest models? What improvements do you observe?\n",
        "- **Performance Comparison**: Compare the accuracy, precision, recall, and F1-scores between:\n",
        "  - The Optuna-optimized Random Forest on the whole dataset\n",
        "  - The aggregated performance of cluster-based Random Forest models\n",
        "  - Which approach performs better overall and why?\n",
        "- **Optimization Strategy**:\n",
        "  - How might the optimal hyperparameters differ between the whole dataset approach vs. individual clusters?\n",
        "- **Methodology Evaluation**: Given these results, would you recommend using Optuna for Random Forest hyperparameter optimization in future medical classification tasks? Justify your answer based on the performance improvements observed."
      ],
      "metadata": {
        "id": "5hUFklZ9viTb"
      },
      "id": "5hUFklZ9viTb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "dH1ZlrTLvmsH"
      },
      "id": "dH1ZlrTLvmsH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Support Vector Machine with Optuna"
      ],
      "metadata": {
        "id": "MMuxdD4ivnh2"
      },
      "id": "MMuxdD4ivnh2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For SVM, we need to optimize several key hyperparameters:\n",
        "\n",
        "**Core Parameters:**\n",
        "- **C**: Regularization parameter (controls the trade-off between margin and classification error)\n",
        "- **gamma**: Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels\n",
        "- **kernel**: Kernel type to be used in the algorithm\n",
        "\n",
        "**Advanced Parameters:**\n",
        "- **degree**: Degree of polynomial kernel (only for 'poly' kernel)\n",
        "\n",
        "**Kernel-Specific Considerations:**\n",
        "- **RBF**: Most commonly used, works well with default gamma='scale'\n",
        "- **Polynomial**: Good for data with polynomial relationships\n",
        "- **Sigmoid**: Similar to RBF but with different mathematical properties\n"
      ],
      "metadata": {
        "id": "V-OBq5Ptvpkf"
      },
      "id": "V-OBq5Ptvpkf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement Optuna Optimization for SVM"
      ],
      "metadata": {
        "id": "1H8PnJR5vrox"
      },
      "id": "1H8PnJR5vrox"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to implement Optuna optimization for SVM with the following requirements:\n",
        "\n",
        "**Parameter Grid to Optimize:**\n",
        "- **C**: categorial([0.1, 1, 10, 50])\n",
        "- **gamma**: categorical(['scale', 'auto', 0.01, 0.1])\n",
        "- **kernel**: categorical(['rbf', 'poly'])\n",
        "- **degree**: categorical([2, 3])\n",
        "\n",
        "**Implementation Setup:**\n",
        "1. Define the objective function with proper parameter handling\n",
        "3. Cross-Validation: Use 3-fold cross-validation with F1-score as the optimization metric\n",
        "4. Study Creation: Create an Optuna study with TPE sampler for 7 trials"
      ],
      "metadata": {
        "id": "xsE3iiOuvuG5"
      },
      "id": "xsE3iiOuvuG5"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "5_5Xw5b_vwSM"
      },
      "id": "5_5Xw5b_vwSM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and Evaluate the Best SVM Model"
      ],
      "metadata": {
        "id": "1Vb4S8-evxqc"
      },
      "id": "1Vb4S8-evxqc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After finding the optimal hyperparameters through Optuna, we need to train and evaluate the Optuna-optimized SVM model with best parameters from the Optuna study and then calculate metrics and display the results. Also plot the confusion matrix."
      ],
      "metadata": {
        "id": "DGunRWBNv073"
      },
      "id": "DGunRWBNv073"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "uzY--Hovv2ud"
      },
      "id": "uzY--Hovv2ud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">SVM with Optuna vs Cluster-Based Classification:\n",
        "\n",
        "Based on the SVM results, answer the following questions by comparing with the cluster-based SVM results from the previous sections:\n",
        "\n",
        "- **Hyperparameter Optimization Impact**: How does using Optuna for hyperparameter optimization compare to using default parameters in the cluster-based SVM models? What improvements do you observe?\n",
        "- **Performance Comparison**: Compare the accuracy, precision, recall, and F1-scores between:\n",
        "  - The Optuna-optimized SVM on the whole dataset\n",
        "  - The aggregated performance of cluster-based SVM models\n",
        "  - Which approach performs better overall and why?\n",
        "- **Optimization Strategy**:\n",
        "  - How might the optimal hyperparameters differ between the whole dataset approach vs. individual clusters?\n",
        "- **Methodology Evaluation**: Given these results, would you recommend using Optuna for SVM hyperparameter optimization in future medical classification tasks? Justify your answer based on the performance improvements observed."
      ],
      "metadata": {
        "id": "z5npptOqv3wS"
      },
      "id": "z5npptOqv3wS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "NkYqdxxzv5vs"
      },
      "id": "NkYqdxxzv5vs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. XGBoost with Optuna"
      ],
      "metadata": {
        "id": "Ux5EfxIWv8Ad"
      },
      "id": "Ux5EfxIWv8Ad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For XGBoost, we need to optimize several key hyperparameters:\n",
        "\n",
        "**Core Parameters:**\n",
        "- **n_estimators**: Number of boosting rounds\n",
        "- **max_depth**: Maximum depth of individual trees\n",
        "- **learning_rate**: Step size shrinkage used to prevent overfitting\n",
        "\n",
        "**Advanced Parameters:**\n",
        "- **subsample**: Subsample ratio of training instances\n",
        "- **colsample_bytree**: Subsample ratio of columns when constructing each tree\n",
        "- **gamma**: Minimum loss reduction required to make a further partition\n"
      ],
      "metadata": {
        "id": "rWtP3J6ov_Ek"
      },
      "id": "rWtP3J6ov_Ek"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement Optuna Optimization for XGBoost"
      ],
      "metadata": {
        "id": "lMUNU3M-wAtw"
      },
      "id": "lMUNU3M-wAtw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to implement Optuna optimization for XGBoost with the following requirements:\n",
        "\n",
        "**Parameter Grid to Optimize:**\n",
        "- **n_estimators**: randint(50, 300)\n",
        "- **max_depth**: randint(3, 10)\n",
        "- **learning_rate**: uniform(0.01, 0.3)\n",
        "- **subsample**: uniform(0.6, 1.0)\n",
        "- **colsample_bytree**: uniform(0.6, 1.0)\n",
        "- **gamma**: uniform(0, 0.5)\n",
        "\n",
        "**Implementation Setup:**\n",
        "1. Define the objective function with proper parameter handling\n",
        "3. Cross-Validation: Use 3-fold cross-validation with F1-score as the optimization metric\n",
        "4. Study Creation: Create an Optuna study with TPE sampler for 15 trials"
      ],
      "metadata": {
        "id": "HxeeEYKpwBWZ"
      },
      "id": "HxeeEYKpwBWZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "OqxAltILwExI"
      },
      "id": "OqxAltILwExI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and Evaluate the Best XGBoost Model"
      ],
      "metadata": {
        "id": "9siakn-IwGNa"
      },
      "id": "9siakn-IwGNa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After finding the optimal hyperparameters through Optuna, we need to train and evaluate the Optuna-optimized XGBoost model with best parameters from the Optuna study and then calculate metrics and display the results. Also plot the confusion matrix."
      ],
      "metadata": {
        "id": "IVEC4AppwIfb"
      },
      "id": "IVEC4AppwIfb"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "3EM_NvIFwKcc"
      },
      "id": "3EM_NvIFwKcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">XGBoost with Optuna vs Cluster-Based Classification:\n",
        "\n",
        "Based on the XGBoost results, answer the following questions by comparing with the cluster-based XGboost results from the previous sections:\n",
        "\n",
        "- **Hyperparameter Optimization Impact**: How does using Optuna for hyperparameter optimization compare to using default parameters in the cluster-based XGBoost models? What improvements do you observe?\n",
        "- **Performance Comparison**: Compare the accuracy, precision, recall, and F1-scores between:\n",
        "  - The Optuna-optimized XGBoost on the whole dataset\n",
        "  - The aggregated performance of cluster-based XGBoost models\n",
        "  - Which approach performs better overall and why?\n",
        "- **Optimization Strategy**:\n",
        "  - How might the optimal hyperparameters differ between the whole dataset approach vs. individual clusters?\n",
        "- **Methodology Evaluation**: Given these results, would you recommend using Optuna for XGBoost hyperparameter optimization in future medical classification tasks? Justify your answer based on the performance improvements observed."
      ],
      "metadata": {
        "id": "DPYS72ftwLyU"
      },
      "id": "DPYS72ftwLyU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "Ds-odvy7wN9r"
      },
      "id": "Ds-odvy7wN9r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison and Analysis\n"
      ],
      "metadata": {
        "id": "Z9EqmEujwRTg"
      },
      "id": "Z9EqmEujwRTg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we perform a high-level comparison of model performance:\n",
        "\n",
        "1. **Whole Dataset Models:**  \n",
        "   Evaluate and compare models (Logistic Regression, Random Forest, SVM, XGBoost) trained on the full dataset.\n",
        "\n",
        "2. **Cluster-Based Models:**  \n",
        "   Assess models trained separately on each cluster and compare their averaged metrics against whole-dataset models.\n",
        "\n",
        "3. **Visualization:**  \n",
        "   Summarize and plot F1-scores and other key metrics to analyze differences between whole-dataset and cluster-based approaches.\n"
      ],
      "metadata": {
        "id": "4nkTLvSCwS5z"
      },
      "id": "4nkTLvSCwS5z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Collect and Organize Results\n",
        "\n"
      ],
      "metadata": {
        "id": "a0OU5hRQwdpk"
      },
      "id": "a0OU5hRQwdpk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For each model trained on the whole dataset:\n",
        "  - Compare predicted labels against the true labels for the entire dataset.\n",
        "  - Compute standard classification metrics: Accuracy, F1 score, Precision, and Recall.\n",
        "  - Store the results in a structured table with a column indicating these are whole-dataset results.\n",
        "\n",
        "- For the cluster-based approach:\n",
        "  - Ensure the existing evaluation results are in a similar structured table.\n",
        "  - Add a column to indicate that these are cluster-based results.\n",
        "\n",
        "- Combine both tables into a single table to facilitate comparison of all models across both approaches."
      ],
      "metadata": {
        "id": "WcxOn3ahwWC3"
      },
      "id": "WcxOn3ahwWC3"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "Zeuqwc8cwYAI"
      },
      "id": "Zeuqwc8cwYAI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create Comprehensive Comparison Plots\n",
        "\n"
      ],
      "metadata": {
        "id": "bqslXNflwbl1"
      },
      "id": "bqslXNflwbl1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Prepare a table where each row contains the F1 score for a specific model and evaluation type (Whole Dataset vs Cluster-Based).\n",
        "\n",
        "- Create a grouped barplot:\n",
        "  - X-axis: Model names (e.g., XGBoost, SVM, Random Forest, Logistic Regression).\n",
        "  - Y-axis: F1 score.\n",
        "  - Use different colors to distinguish between Whole Dataset and Cluster-Based evaluation."
      ],
      "metadata": {
        "id": "zwsHxeGywYVP"
      },
      "id": "zwsHxeGywYVP"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "yaQ3UPoVwfka"
      },
      "id": "yaQ3UPoVwfka",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=\"#ffcc00\">**Final Analysis Questions:**\n",
        "\n",
        "Based on your comprehensive analysis, answer the following questions:\n",
        "\n",
        "- **Model Performance Analysis**: Which model performed best overall? What factors contributed to its success?\n",
        "\n",
        "- **Approach Comparison**: How do the pure classification results compare with the cluster-based approach? Which method would you recommend for this diabetes prediction task?\n",
        "\n",
        "- **Medical Application**: Considering this is a medical classification task, which approach would be most suitable for deployment in a healthcare setting? Consider factors like interpretability, reliability, and clinical utility.\n",
        "\n",
        "- **Future Improvements**: What additional techniques or modifications would you suggest to further improve the classification performance?\n"
      ],
      "metadata": {
        "id": "ytScoKPawgSJ"
      },
      "id": "ytScoKPawgSJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <font color='green'>Your Answer:</font></b><br>\n",
        "\n",
        "\n",
        "</p>\n",
        "</font>\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "872YvaQWwiyi"
      },
      "id": "872YvaQWwiyi"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "brain_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}